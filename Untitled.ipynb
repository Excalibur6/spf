{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrizz_000\\anaconda3\\anaconda3\\envs\\learn-env\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "In C:\\Users\\jrizz_000\\anaconda3\\anaconda3\\envs\\learn-env\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\jrizz_000\\anaconda3\\anaconda3\\envs\\learn-env\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\jrizz_000\\anaconda3\\anaconda3\\envs\\learn-env\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\jrizz_000\\anaconda3\\anaconda3\\envs\\learn-env\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\jrizz_000\\anaconda3\\anaconda3\\envs\\learn-env\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\jrizz_000\\anaconda3\\anaconda3\\envs\\learn-env\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\jrizz_000\\anaconda3\\anaconda3\\envs\\learn-env\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\jrizz_000\\anaconda3\\anaconda3\\envs\\learn-env\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "#**************** IMPORT PACKAGES ********************\n",
    "from flask import Flask, render_template, request, flash, redirect, url_for\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import datetime\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import math, random\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "import json\n",
    "import yfinance as yf\n",
    "import tweepy\n",
    "import preprocessor as p\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "import xgboost as xgboost\n",
    "import simplejson\n",
    "from collections import deque\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from textblob import TextBlob\n",
    "import constants as ct\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical(quote):\n",
    "    end = datetime.now()\n",
    "    start = datetime(end.year-2,end.month,end.day)\n",
    "    data = yf.download(quote, start=start, end=end)\n",
    "    df = pd.DataFrame(data=data)\n",
    "    df.to_csv(''+quote+'.csv')\n",
    "    if(df.empty):\n",
    "        from alpha_vantage.timeseries import TimeSeries\n",
    "        ts = TimeSeries(key='N6A6QT6IBFJOPJ70',output_format='pandas')\n",
    "        data, meta_data = ts.get_daily_adjusted(symbol='NSE:'+quote, outputsize='full')\n",
    "        #Format df\n",
    "        #Last 2 yrs rows => 502, in ascending order => ::-1\n",
    "        data=data.head(503).iloc[::-1]\n",
    "        data=data.reset_index()\n",
    "        #Keep Required cols only\n",
    "        df=pd.DataFrame()\n",
    "        df['Date']=data['date']\n",
    "        df['Open']=data['1. open']\n",
    "        df['High']=data['2. high']\n",
    "        df['Low']=data['3. low']\n",
    "        df['Close']=data['4. close']\n",
    "        df['Adj Close']=data['5. adjusted close']\n",
    "        df['Volume']=data['6. volume']\n",
    "        df.to_csv(''+quote+'.csv',index=False)\n",
    "    return\n",
    "\n",
    "#******************** ARIMA SECTION ********************\n",
    "def ARIMA_ALGO(df):\n",
    "    from pmdarima.arima import auto_arima\n",
    "    from scipy.ndimage.interpolation import shift\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    import json\n",
    "    uniqueVals = df[\"symbol\"].unique()  \n",
    "    len(uniqueVals)\n",
    "    df=df.set_index(\"symbol\")\n",
    "    #for daily bas'is\n",
    "    def parser(x):\n",
    "        from datetime import datetime\n",
    "        return datetime.strptime(x, '%Y-%m-%d')\n",
    "    d['Date'] = pd.to_datetime(d['date']).map(lambda x: x.date())\n",
    "    date = d.Date\n",
    "    price = d.adjClose\n",
    "    for company in uniqueVals[:10]:\n",
    "        data=(df.loc[company,:]).reset_index() \n",
    "        data[['Code','Open','Low','High','Close','Adj_close','Date']] = data[['symbol','open','low','high','close', 'adjClose','date']]\n",
    "        Quantity_date = data[['Code','Open','Low','High','Close','Adj_close', 'Date']]\n",
    "\n",
    "        Quantity_date['Date'] = pd.to_datetime(Quantity_date['Date']).map(lambda x: x.date())\n",
    "        Quantity_date = Quantity_date.fillna(Quantity_date.bfill())\n",
    "\n",
    "        code = Quantity_date['Code'].to_list()\n",
    "        close=Quantity_date['Close'].to_list()\n",
    "        date=Quantity_date['Date'].to_list()\n",
    "        open=Quantity_date['Open'].to_list()\n",
    "        high=Quantity_date['High'].to_list()\n",
    "        low=Quantity_date['Low'].to_list()\n",
    "        Adj_close=Quantity_date['Adj_close'].to_list()\n",
    "\n",
    "        close = pd.DataFrame(data = close, columns=[\"Close\"])\n",
    "        code = pd.DataFrame(data = code, columns=['Code'])\n",
    "        date = pd.DataFrame(data = date, columns=[\"Date\"]).astype(str)\n",
    "        open = pd.DataFrame(data = open, columns=[\"Open\"])\n",
    "        high = pd.DataFrame(data = high, columns=[\"High\"])\n",
    "        low = pd.DataFrame(data = low, columns=[\"Low\"])\n",
    "        Adj_close = pd.DataFrame(data = Adj_close, columns=[\"Adj_close\"])\n",
    "\n",
    "        result = pd.concat([code, date, open, low, high, close, Adj_close], axis=1, ignore_index=True)\n",
    "\n",
    "        result.columns = ['Code','Date','Open','Low','High','Close','Adj_close']\n",
    "\n",
    "        Quantity_date = Quantity_date.drop(['Code','Date','Open','Low','High','Close'],axis =1)\n",
    "\n",
    "        print()\n",
    "        fig = plt.figure(figsize=(7.2,4.8),dpi=65)\n",
    "        plt.plot(Quantity_date)\n",
    "        plt.savefig('Trends.png')\n",
    "        plt.close(fig)\n",
    "        #plt.show()\n",
    "\n",
    "        quantity = Quantity_date.values\n",
    "        size = int(len(quantity) * 0.65)\n",
    "        train, test = quantity[0:size], quantity[size:len(quantity)]\n",
    "        #fit in model\n",
    "\n",
    "        def arima_model(train, test):\n",
    "            history = [x for x in train]\n",
    "            predictions = [x for x in train]\n",
    "            onlypreds = []\n",
    "            for t in range(len(test)+7):\n",
    "                model = ARIMA(history, order=(6,1 ,0))\n",
    "                model = model.fit(disp=0)\n",
    "                output = model.forecast()\n",
    "                output = pd.DataFrame(output)\n",
    "                yhat = output[0]\n",
    "                predictions.append(yhat[0])\n",
    "                onlypreds.append(yhat[0])\n",
    "                if t < len(test):\n",
    "                    obs = test[t]\n",
    "                    history.append(obs)\n",
    "                else:\n",
    "                    obs = yhat[0]\n",
    "                    history.append(obs)\n",
    "            return predictions, onlypreds\n",
    "\n",
    "        preds, onlypreds = arima_model(train, test)\n",
    "\n",
    "        error_arima = math.sqrt(mean_squared_error(test, onlypreds[0:len(test)]))\n",
    "\n",
    "        result[\"Date\"] = result['Date'].astype(str).str.replace(\"-\",\"/\")\n",
    "        x = np.append(train, onlypreds)\n",
    "\n",
    "        pre = pd.DataFrame(x, columns=[\"ARIMA\"])\n",
    "        pre = pd.concat([pre, result['Adj_close']], axis=1)\n",
    "        pre = pd.concat([pre, result['Date']], axis=1)\n",
    "\n",
    "        idx = pd.date_range(np.array(result.Date)[-1], periods=8, freq='D')\n",
    "        pre.Date[-8:] = idx.map(lambda x: x.date()).astype(str).str.replace(\"-\",\"/\")\n",
    "\n",
    "\n",
    "        #plot graph\n",
    "        print()\n",
    "        #print(\"ARIMA model Accuracy: \")\n",
    "        fig = plt.figure(figsize=(7.2,4.8),dpi=65)\n",
    "        plt.plot(result['Adj_close'], label='History')\n",
    "        plt.plot(pre['Date'], pre[\"ARIMA\"], label='Predicted')\n",
    "        plt.legend(loc=4)\n",
    "        plt.savefig('ARIMA.png')\n",
    "        plt.close(fig)\n",
    "\n",
    "        arima_test=quantity\n",
    "        arima_predi=preds\n",
    "        tomorrow_ar = arima_predi[-7]\n",
    "\n",
    "\n",
    "        #plt.show()\n",
    "        print()\n",
    "        print(\"####arima_predi##########################################################################\")\n",
    "        print(\"Tomorrow's\",quote,\" Closing Price Prediction by ARIMA:\",tomorrow_ar )\n",
    "        print(\"ARIMA RMSE:\",error_arima)\n",
    "        print(\"##############################################################################\")\n",
    "        print()\n",
    "        prices = {\"Date\": date, \"History\": price, \"Forecast\": arima_predi}\n",
    "\n",
    "        return arima_predi, error_arima, tomorrow_ar, result, pre\n",
    "\n",
    "\n",
    "#******************** LSTM SECTION ********************\n",
    "def LSTM_ALGO(d):\n",
    "    n = 100\n",
    "    df1=d.reset_index()['close']\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler=MinMaxScaler(feature_range=(0,1))\n",
    "    df1=scaler.fit_transform(np.array(df1).reshape(-1,1))\n",
    "\n",
    "    ##splitting dataset into train and test split\n",
    "    training_size=int(len(df1)*0.65)\n",
    "    test_size=len(df1)-training_size\n",
    "    train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]\n",
    "\n",
    "    import numpy\n",
    "    # convert an array of values into a dataset matrix\n",
    "    def create_dataset(dataset, time_step=1):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-time_step-1):\n",
    "            a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 \n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + time_step, 0])\n",
    "        return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "    # reshape into X=t,t+1,t+2,t+3 and Y=t+4\n",
    "    time_step = n\n",
    "    X_train, y_train = create_dataset(train_data, time_step)\n",
    "    X_test, ytest = create_dataset(test_data, time_step)\n",
    "\n",
    "\n",
    "    # reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "    X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "\n",
    "    model=Sequential()\n",
    "    model.add(LSTM(50,return_sequences=True,input_shape=(n,1)))\n",
    "    model.add(Dropout(p=0.1))\n",
    "\n",
    "    #Add 2nd LSTM layer\n",
    "    model.add(LSTM(units=50,return_sequences=True))\n",
    "    model.add(Dropout(p=0.1))\n",
    "\n",
    "    #Add 3rd LSTM layer\n",
    "    model.add(LSTM(units=50,return_sequences=True))\n",
    "    model.add(Dropout(p=0.1))\n",
    "\n",
    "    #Add 4th LSTM layer\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dropout(p=0.1))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "\n",
    "    model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=10,batch_size=100,verbose=1)\n",
    "\n",
    "    ### Lets Do the prediction and check performance metrics\n",
    "    train_predict=model.predict(X_train)\n",
    "    test_predict=model.predict(X_test)\n",
    "\n",
    "    train_predict=scaler.inverse_transform(train_predict)\n",
    "    test_predict=scaler.inverse_transform(test_predict)\n",
    "\n",
    "    import math\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    ### Test Data RMSE\n",
    "    error_lstm = math.sqrt(mean_squared_error(ytest,test_predict))\n",
    "\n",
    "    x_input=test_data[len(test_data)-n:].reshape(1,-1)\n",
    "\n",
    "    temp_input=list(x_input)\n",
    "    temp_input=temp_input[0].tolist()\n",
    "\n",
    "\n",
    "    # demonstrate prediction for next 10 days\n",
    "    from numpy import array\n",
    "\n",
    "    lst_output=[]\n",
    "    n_steps=n\n",
    "    i=0\n",
    "    while(i<7):\n",
    "\n",
    "        if(len(temp_input)>n):\n",
    "            #print(temp_input)\n",
    "            x_input=np.array(temp_input[1:])\n",
    "            print(\"{} day input {}\".format(i,x_input))\n",
    "            x_input=x_input.reshape(1,-1)\n",
    "            x_input = x_input.reshape((1, n_steps, 1))\n",
    "            #print(x_input)\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "            print(\"{} day output {}\".format(i,yhat))\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "            temp_input=temp_input[1:]\n",
    "            #print(temp_input)\n",
    "            lst_output.extend(yhat.tolist())\n",
    "            i=i+1\n",
    "        else:\n",
    "            x_input = x_input.reshape((1, n_steps,1))\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "            print(yhat[0])\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "            print(len(temp_input))\n",
    "            lst_output.extend(yhat.tolist())\n",
    "            i=i+1\n",
    "\n",
    "    df3=df1.tolist()\n",
    "    df3.extend(lst_output)\n",
    "    df3=scaler.inverse_transform(df3).tolist()\n",
    "    plt.plot(df3)\n",
    "    tomorrow_lstm=df3[-7]\n",
    "    return df3, error_lstm, tomorrow_lstm\n",
    "\n",
    "\n",
    "\n",
    "#**************** SENTIMENT ANALYSIS **************************\n",
    "def retrieving_tweets_polarity(symbol):\n",
    "    auth = tweepy.OAuthHandler(ct.consumer_key, ct.consumer_secret)\n",
    "    auth.set_access_token(ct.access_token, ct.access_token_secret)\n",
    "    user = tweepy.API(auth)\n",
    "\n",
    "    tweets = tweepy.Cursor(user.search, q=str(symbol), tweet_mode='extended', lang='en',exclude_replies=True).items(ct.num_of_tweets)\n",
    "\n",
    "    tweet_list = [] #List of tweets alongside polarity\n",
    "    global_polarity = 0 #Polarity of all tweets === Sum of polarities of individual tweets\n",
    "    tw_list=[] #List of tweets only => to be displayed on web page\n",
    "    #Count Positive, Negative to plot pie chart\n",
    "    pos=0 #Num of pos tweets\n",
    "    neg=1 #Num of negative tweets\n",
    "    for tweet in tweets:\n",
    "        count=20 #Num of tweets to be displayed on web page\n",
    "        #Convert to Textblob format for assigning polarity\n",
    "        tw2 = tweet.full_text\n",
    "        tw = tweet.full_text\n",
    "        #Clean\n",
    "        tw=p.clean(tw)\n",
    "        #print(\"-------------------------------CLEANED TWEET-----------------------------\")\n",
    "        #print(tw)\n",
    "        #Replace &amp; by &\n",
    "        tw=re.sub('&amp;','&',tw)\n",
    "        #Remove :\n",
    "        tw=re.sub(':','',tw)\n",
    "        #print(\"-------------------------------TWEET AFTER REGEX MATCHING-----------------------------\")\n",
    "        #print(tw)\n",
    "        #Remove Emojis and Hindi Characters\n",
    "        tw=tw.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "        #print(\"-------------------------------TWEET AFTER REMOVING NON ASCII CHARS-----------------------------\")\n",
    "        #print(tw)\n",
    "        blob = TextBlob(tw)\n",
    "        polarity = 0 #Polarity of single individual tweet\n",
    "        for sentence in blob.sentences:\n",
    "\n",
    "            polarity += sentence.sentiment.polarity\n",
    "            if polarity>0:\n",
    "                pos=pos+1\n",
    "            if polarity<0:\n",
    "                neg=neg+1\n",
    "\n",
    "            global_polarity += sentence.sentiment.polarity\n",
    "        if count > 0:\n",
    "            tw_list.append(tw2)\n",
    "\n",
    "        tweet_list.append(Tweet(tw, polarity))\n",
    "        count=count-1\n",
    "    global_polarity = global_polarity / len(tweet_list)\n",
    "    neutral=ct.num_of_tweets-pos-neg\n",
    "    if neutral<0:\n",
    "        neg=neg+neutral\n",
    "        neutral=20\n",
    "    print()\n",
    "    print(\"##############################################################################\")\n",
    "    print(\"Positive Tweets :\",pos,\"Negative Tweets :\",neg,\"Neutral Tweets :\",neutral)\n",
    "    print(\"##############################################################################\")\n",
    "    print()\n",
    "    labels=['Positive','Negative','Neutral']\n",
    "    sizes = [abs(pos),abs(neg),abs(neutral)]\n",
    "    explode = (0, 0, 0)\n",
    "\n",
    "    pie = pd.DataFrame(sizes, columns = [\"sizes\"])\n",
    "    pie['labels'] = labels\n",
    "    big_data = {\"sizes\": np.array(pie.sizes), \"labels\": np.array(pie.labels)}\n",
    "    df2=pd.DataFrame(big_data)\n",
    "    k = df2.to_dict('records')\n",
    "    out_file = open(\"static/assets/js/dashboard/pie.json\", \"w\", encoding='utf-8') \n",
    "    simplejson.dump(k, out_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(7.2,4.8),dpi=65)\n",
    "    fig1, ax1 = plt.subplots(figsize=(7.2,4.8),dpi=65)\n",
    "    ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "    # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    ax1.axis('equal')  \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('static/SA.png')\n",
    "    plt.close(fig)\n",
    "    #plt.show()\n",
    "    if global_polarity>0:\n",
    "        print()\n",
    "        print(\"##############################################################################\")\n",
    "        print(\"Tweets Polarity: Overall Positive\")\n",
    "        print(\"##############################################################################\")\n",
    "        print()\n",
    "        tw_pol=\"Overall Positive\"\n",
    "    else:\n",
    "        print()\n",
    "        print(\"##############################################################################\")\n",
    "        print(\"Tweets Polarity: Overall Negative\")\n",
    "        print(\"##############################################################################\")\n",
    "        print()\n",
    "        tw_pol=\"Overall Negative\"\n",
    "    return global_polarity,tw_list,tw_pol,pos,neg,neutral\n",
    "\n",
    "\n",
    "\n",
    "def recommending(df, global_polarity,today_stock,mean):\n",
    "    if today_stock.iloc[-1]['adjClose'] < mean:\n",
    "        if global_polarity > 0:\n",
    "            print()\n",
    "\n",
    "            idea=\"RISE\"\n",
    "            decision=\"BUY\"\n",
    "            print()\n",
    "            print(\"##############################################################################\")\n",
    "            print(\"According to the DL Predictions and Sentiment Analysis of Tweets, a\",idea,\"in\",quote,\"stock is expected: \",decision)\n",
    "        elif global_polarity < 0:\n",
    "            print()\n",
    "            idea=\"FALL\"\n",
    "            decision=\"SELL\"\n",
    "            print()\n",
    "            print(\"##############################################################################\")\n",
    "            print(\"According to the DL Predictions and Sentiment Analysis of Tweets, a\",idea,\"in\",quote,\"stock is expected: \",decision)\n",
    "    else:\n",
    "        print()\n",
    "        idea=\"FALL\"\n",
    "        decision=\"SELL\"\n",
    "        print()\n",
    "        print(\"##############################################################################\")\n",
    "        print(\"According to the DL Predictions and Sentiment Analysis of Tweets, a\",idea,\"in\",quote,\"stock is expected: \",decision)\n",
    "    return idea, decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader as pdr\n",
    "key=\"60104c377746149d341afb340c95833238fe5e73\"\n",
    "d = pdr.get_data_tiingo(quote, api_key=key)\n",
    "d.dropna().to_csv(quote+'.csv')\n",
    "d=pd.read_csv(quote+'.csv')\n",
    "d = d[500:]\n",
    "today_stock=d.iloc[-1:]\n",
    "print(\"##############################################################################\")\n",
    "print(\"Today's\",quote,\"Stock Data: \")\n",
    "print(today_stock)\n",
    "\n",
    "plt.plot(d['close'])\n",
    "\n",
    "#predictions\n",
    "arima_predi, error_arima, tomorrow_ar, i, pre = ARIMA_ALGO(d)\n",
    "df3, error_lstm, tomorrow_lstm = LSTM_ALGO(d)\n",
    "\n",
    "df3 = pd.DataFrame(df3, columns = [\"LSTM\"])\n",
    "df3 = pd.concat([df3, pre.Adj_close], axis=1)\n",
    "df3 = pd.concat([df3, pre.Date], axis=1)\n",
    "all_pred = pd.concat([df3, pre[\"ARIMA\"]], axis=1)\n",
    "\n",
    "print()\n",
    "#print(\"Recent %s related Tweets & News: \" % quote)\n",
    "polarity,tw_list,tw_pol,pos,neg,neutral = retrieving_tweets_polarity(quote)\n",
    "dates = np.array(all_pred[\"Date\"].tail(7)).reshape(-1,1)\n",
    "print(\"ARIMA Model Forecasted Prices for Next 7 days:\")\n",
    "forecast_set_ar = np.round(np.array(all_pred[\"ARIMA\"].tail(7)),2).reshape(-1,1)\n",
    "mean=d[\"adjClose\"].tail(7).mean()\n",
    "print(forecast_set_ar)\n",
    "print(\"LSTM Forecasted Prices for Next 7 days:\")\n",
    "forecast_set_ls = np.round(np.array(all_pred[\"LSTM\"].tail(7)), 2).reshape(-1,1)\n",
    "print(forecast_set_ls)\n",
    "print()\n",
    "#print(\"Generating recommendation based on prediction & polarity...\")\n",
    "idea, decision=recommending(i, polarity,today_stock,mean)\n",
    "today_stock=today_stock.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
